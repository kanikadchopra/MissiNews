{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NewsHeadlines.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ft2ojofV2ciQ",
        "RYGcEIR-NgMV",
        "oUMgEdBMVcVV",
        "3AHZUh32X9Tr"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGIcB2GG0HSM",
        "colab_type": "text"
      },
      "source": [
        "# News Classification \n",
        "\n",
        "By: Kanika Chopra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtqhZyDlR_JK",
        "colab_type": "code",
        "outputId": "baf4ddd5-b350-4a02-a67f-e9a0cd680392",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install contractions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.5)\n",
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 1.7MB/s \n",
            "\u001b[?25hCollecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81703 sha256=b63018296cff7857e1365e13a6b9dbec66a7153987a801ff24565868d35536d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RecLLN9_T83r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# NLP Preprocessing \n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Models \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Features \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Parameter Tuning and Evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score, classification_report, make_scorer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "\n",
        "import string\n",
        "import contractions\n",
        "import re\n",
        "\n",
        "from nlp import get_pos\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I2ipeJ30GEs",
        "colab_type": "text"
      },
      "source": [
        "## The Problem\n",
        "\n",
        "I will be using a supervised machine learning algorithm using a dataset from Kaggle with Huffpost news articles and their categories. \n",
        "\n",
        "The problem is to train a classifier to classify news articles based on their headlines and then understand the success of using transfer learning to apply this model to news-related tweets. \n",
        "\n",
        "First, let's begin with some data analysis. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv9qroIQ4HX1",
        "colab_type": "text"
      },
      "source": [
        "## Data Analysis \n",
        "\n",
        "Let's take a look at the data we have, this will help us determine what type of data preprocessing needs to be conducted before we can start feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biRN5r3PUmeD",
        "colab_type": "code",
        "outputId": "e53a119d-57c6-410e-9c6f-0015efdf2330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df = pd.read_json(\"News_Category_Dataset_v2.json\", lines = True)\n",
        "\n",
        "# What columns do we have \n",
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['category', 'headline', 'authors', 'link', 'short_description', 'date'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqq-FQG3VfzZ",
        "colab_type": "code",
        "outputId": "0fdea701-d804-4084-eb42-15f36e07683b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "# Categories Distribution\n",
        "df['category'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "POLITICS          32739\n",
              "WELLNESS          17827\n",
              "ENTERTAINMENT     16058\n",
              "TRAVEL             9887\n",
              "STYLE & BEAUTY     9649\n",
              "PARENTING          8677\n",
              "HEALTHY LIVING     6694\n",
              "QUEER VOICES       6314\n",
              "FOOD & DRINK       6226\n",
              "BUSINESS           5937\n",
              "COMEDY             5175\n",
              "SPORTS             4884\n",
              "BLACK VOICES       4528\n",
              "HOME & LIVING      4195\n",
              "PARENTS            3955\n",
              "THE WORLDPOST      3664\n",
              "WEDDINGS           3651\n",
              "WOMEN              3490\n",
              "IMPACT             3459\n",
              "DIVORCE            3426\n",
              "CRIME              3405\n",
              "MEDIA              2815\n",
              "WEIRD NEWS         2670\n",
              "GREEN              2622\n",
              "WORLDPOST          2579\n",
              "RELIGION           2556\n",
              "STYLE              2254\n",
              "SCIENCE            2178\n",
              "WORLD NEWS         2177\n",
              "TASTE              2096\n",
              "TECH               2082\n",
              "MONEY              1707\n",
              "ARTS               1509\n",
              "FIFTY              1401\n",
              "GOOD NEWS          1398\n",
              "ARTS & CULTURE     1339\n",
              "ENVIRONMENT        1323\n",
              "COLLEGE            1144\n",
              "LATINO VOICES      1129\n",
              "CULTURE & ARTS     1030\n",
              "EDUCATION          1004\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmfq2uhC46HW",
        "colab_type": "code",
        "outputId": "0cb7f0bf-0a81-4fe5-fb53-fa3d972ef235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Number of categories\n",
        "len(df['category'].value_counts())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZO7xe-9VlXO",
        "colab_type": "code",
        "outputId": "a8c0fbf7-5adf-4cdc-d3d7-e28f69309821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Let's view our data\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>headline</th>\n",
              "      <th>authors</th>\n",
              "      <th>link</th>\n",
              "      <th>short_description</th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
              "      <td>Melissa Jeltsen</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
              "      <td>She left her husband. He killed their children...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
              "      <td>Andy McDonald</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
              "      <td>Of course it has a song.</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
              "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
              "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
              "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
              "      <td>2018-05-26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category  ...       date\n",
              "0          CRIME  ... 2018-05-26\n",
              "1  ENTERTAINMENT  ... 2018-05-26\n",
              "2  ENTERTAINMENT  ... 2018-05-26\n",
              "3  ENTERTAINMENT  ... 2018-05-26\n",
              "4  ENTERTAINMENT  ... 2018-05-26\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46Jl2YCpW_lS",
        "colab_type": "code",
        "outputId": "db26e321-eb24-417c-9901-66de8fcf3f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# How long is our dataset\n",
        "len(df), len(df.columns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200853, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHvqp3VDXE_t",
        "colab_type": "text"
      },
      "source": [
        "We have 200,853 news headlines and 6 columns. Our classifier is going to be built on the headlines column to predict category. \n",
        "\n",
        "Before we begin data preprocessing, we can see that we have a lot of categories, 41 specifically, I want to first focus on training a subset of this data. Primarily, the distinct topics that I am most interested in seeing are: Politics, Sports, Entertainment Business and Crime (for simplicity)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gfC_MVSXfW7",
        "colab_type": "code",
        "outputId": "a7b85140-c223-4c5e-ad84-104296217026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "news = df[df['category'].isin(['CRIME', 'ENTERTAINMENT', 'POLITICS', 'SPORTS', 'BUSINESS'])]\n",
        "\n",
        "# Remove unnecessary columns\n",
        "news = news[['date', 'authors', 'headline', 'category', 'link']]\n",
        "\n",
        "news.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>authors</th>\n",
              "      <th>headline</th>\n",
              "      <th>category</th>\n",
              "      <th>link</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>Melissa Jeltsen</td>\n",
              "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
              "      <td>CRIME</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>Andy McDonald</td>\n",
              "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-05-26</td>\n",
              "      <td>Ron Dicker</td>\n",
              "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        date  ...                                               link\n",
              "0 2018-05-26  ...  https://www.huffingtonpost.com/entry/texas-ama...\n",
              "1 2018-05-26  ...  https://www.huffingtonpost.com/entry/will-smit...\n",
              "2 2018-05-26  ...  https://www.huffingtonpost.com/entry/hugh-gran...\n",
              "3 2018-05-26  ...  https://www.huffingtonpost.com/entry/jim-carre...\n",
              "4 2018-05-26  ...  https://www.huffingtonpost.com/entry/julianna-...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0gfTUYw3R9y",
        "colab_type": "code",
        "outputId": "9e0321a1-0e80-4e70-f3fe-327bc5405f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "news['category'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "POLITICS         32739\n",
              "ENTERTAINMENT    16058\n",
              "BUSINESS          5937\n",
              "SPORTS            4884\n",
              "CRIME             3405\n",
              "Name: category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47m5xHPA3Rey",
        "colab_type": "text"
      },
      "source": [
        "We can see that the class distribution is highly skewed with politics having over 30,000 headlines vs. crime having less than 3500 headlines. This means our classifier might be really good with classifying politics-related articles, but less strong with crime-related articles. \n",
        "\n",
        "First, we will train our model and look at the confusion matrix to see if we need to take further steps to handle the imbalanced dataset but we will keep this in the back of our mind for now.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ix-1ntp50CL",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "We want to use bag of words and tf-idf for our features so before we do that, we need to complete some preprocessing steps. Below are the steps we will be taking:\n",
        "\n",
        "*   Break apart contractions\n",
        "\n",
        "*   Make all headlines lowercase\n",
        "*   Convert all numbers to the string 'num'\n",
        "*   Remove punctuation (replace with empty string) \n",
        "*   Remove all stop words\n",
        "*   Lemmatize all words (back to their root words)\n",
        "*   Combine the list of words into a string again\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FV2ZenSgjFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Break apart all contractions (except name possession e.g. Sarah's)\n",
        "news['headline'] = news['headline'].apply(lambda x: contractions.fix(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcDVPPNbgjPf",
        "colab_type": "code",
        "outputId": "b157bf3a-53c3-4274-c1bc-0e554a65a5b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Download stop words \n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKZgL3uBhDF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert to lowercase \n",
        "news['clean_headline'] = news['headline'].apply(lambda x: x.lower())\n",
        "\n",
        "# Convert all numbers in the headlines to the word 'num' using re\n",
        "news['clean_headline'] = news['clean_headline'].apply(lambda x: re.sub(r'\\d+', 'num', x))\n",
        "\n",
        "# Remove punctuation\n",
        "punct = str.maketrans('', '', string.punctuation)\n",
        "news['clean_headline'] = news['clean_headline'].apply(lambda x: x.translate(punct))\n",
        "\n",
        "# Initialize tokenizer so that it doesn't include punctuation\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "news['clean_headline'] = [tokenizer.tokenize(x) for x in news['clean_headline']]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "news['clean_headline'] = news['clean_headline'].apply(lambda x: [word for word in x if word not in stop_words])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CFrrg_YeM7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize the headlines\n",
        "news['clean_headline'] = news['clean_headline'].apply(lambda x: [lemmatizer.lemmatize(word, get_pos(word)) for word in x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJvKlM567PKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the list of words into a string\n",
        "news['clean_headline'] = news['clean_headline'].apply(lambda x: ' '.join(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B08fFEgkewkt",
        "colab_type": "code",
        "outputId": "7e205fb7-061c-480a-805e-cb1fce6f82a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Let's compare the before and after\n",
        "print(news['headline'][0])\n",
        "print(news['clean_headline'][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV\n",
            "num mass shooting texas last week num tv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEv5FbKF6MI3",
        "colab_type": "text"
      },
      "source": [
        "## Feature Extraction\n",
        "\n",
        "We are going to have a bag-of-words model and a tf-idf matrix for our features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsAakVgh7imi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate bag of words object with maximum vocab size of 1000\n",
        "vec_counter = CountVectorizer(max_features = 1000)\n",
        "\n",
        "# Get bag of words model as sparse matrix\n",
        "bag_of_words = vec_counter.fit_transform(news['clean_headline'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmP5kvRq8RqB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate our tf-idf object with maximum vocab size of 1000\n",
        "tf_counter = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Get tf-idf matrix as sparse matrix\n",
        "tfidf = tf_counter.fit_transform(news['clean_headline'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJMhMETUy9g-",
        "colab_type": "code",
        "outputId": "91031122-d00b-4799-c9d6-88d2efc0d653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# Get a preview of the words corresponding to the vocab index \n",
        "tf_counter.get_feature_names()[:25]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abortion',\n",
              " 'abuse',\n",
              " 'access',\n",
              " 'accuse',\n",
              " 'act',\n",
              " 'action',\n",
              " 'activist',\n",
              " 'actor',\n",
              " 'actually',\n",
              " 'ad',\n",
              " 'adam',\n",
              " 'address',\n",
              " 'administration',\n",
              " 'admits',\n",
              " 'adorable',\n",
              " 'adviser',\n",
              " 'age',\n",
              " 'agency',\n",
              " 'ago',\n",
              " 'ahead',\n",
              " 'aide',\n",
              " 'aim',\n",
              " 'air',\n",
              " 'al',\n",
              " 'alabama']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epRH7-9uY3HZ",
        "colab_type": "code",
        "outputId": "2493ae35-97a4-49a8-c1a6-ffa9bb5ba0b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tfidf.toarray().shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63023, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF3j3_eoZEAm",
        "colab_type": "text"
      },
      "source": [
        "This means that each of the 63,023 headlies is represented with 1000 features representing the tf-idf score for different unigrams and bigrams. \n",
        "\n",
        "This step was moreso to get an idea of how and what CountVectorizer and TD-IDF were doing, but these steps will be added into a pipeline when training our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-OGYx9F896j",
        "colab_type": "text"
      },
      "source": [
        "## Training and Testing Dataset\n",
        "\n",
        "We are going to split our data into 80% training data, and 20% testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imbGbxrgAwC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(news['clean_headline'], news['category'],\n",
        "                                                    test_size=0.2, random_state=42, stratify=news['category'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPVnP241ClKV",
        "colab_type": "code",
        "outputId": "fe81c7cc-a03e-48b0-b04c-b1846ba7e488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Check the distribution of classes in the training and testing dataset\n",
        "y_distr = pd.DataFrame(y_train.value_counts())\n",
        "y_distr.reset_index(inplace=True)\n",
        "y_distr = y_distr.merge(y_test.value_counts().to_frame().reset_index(), how='inner', on='index')\n",
        "\n",
        "y_distr.columns = ['category', 'train', 'test']\n",
        "\n",
        "# Add percentages of testing and training \n",
        "y_distr['train pct'] = y_distr['train'].apply(lambda x: x/sum(y_distr['train']))\n",
        "y_distr['test pct'] = y_distr['test'].apply(lambda x: x/sum(y_distr['test']))\n",
        "\n",
        "y_distr"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>train</th>\n",
              "      <th>test</th>\n",
              "      <th>train pct</th>\n",
              "      <th>test pct</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POLITICS</td>\n",
              "      <td>26191</td>\n",
              "      <td>6548</td>\n",
              "      <td>0.519477</td>\n",
              "      <td>0.519476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ENTERTAINMENT</td>\n",
              "      <td>12846</td>\n",
              "      <td>3212</td>\n",
              "      <td>0.254790</td>\n",
              "      <td>0.254820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>BUSINESS</td>\n",
              "      <td>4750</td>\n",
              "      <td>1187</td>\n",
              "      <td>0.094212</td>\n",
              "      <td>0.094169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SPORTS</td>\n",
              "      <td>3907</td>\n",
              "      <td>977</td>\n",
              "      <td>0.077492</td>\n",
              "      <td>0.077509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CRIME</td>\n",
              "      <td>2724</td>\n",
              "      <td>681</td>\n",
              "      <td>0.054028</td>\n",
              "      <td>0.054026</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        category  train  test  train pct  test pct\n",
              "0       POLITICS  26191  6548   0.519477  0.519476\n",
              "1  ENTERTAINMENT  12846  3212   0.254790  0.254820\n",
              "2       BUSINESS   4750  1187   0.094212  0.094169\n",
              "3         SPORTS   3907   977   0.077492  0.077509\n",
              "4          CRIME   2724   681   0.054028  0.054026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwwiTAhX3F09",
        "colab_type": "text"
      },
      "source": [
        "We can see that our training and testing distributions are very similarly distributed so now that we have done our preprocessing and have our training and testing sets, we can train our models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFh185Zw0oG6",
        "colab_type": "text"
      },
      "source": [
        "## Building a Pipeline \n",
        "\n",
        "The above code is to implement the bag of words and tfidf individually. We can also create a pipeline that will do these two steps and then also train a model. We will be creating pipelines for the following models:\n",
        "1. Naive Bayes Classifier (NB)\n",
        "2. Support Vector Machine (SVM) \n",
        "* Linear Kernel\n",
        "* Polynomial Kernel\n",
        "* Gaussian Kernel\n",
        "3. Logistic Regression\n",
        "4. Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOc8Lmluzd4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set evaluation scores to use when fine-tuning parameters\n",
        "scorers = {\n",
        "    'precision_score': make_scorer(precision_score, greater_is_better=True, average='micro'),\n",
        "    'recall_score': make_scorer(recall_score, greater_is_better=True, average='micro'),\n",
        "    'accuracy_score': make_scorer(accuracy_score)\n",
        "}\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ft2ojofV2ciQ",
        "colab_type": "text"
      },
      "source": [
        "## 1. Naive Bayes Classifier\n",
        "Let's begin with training a classifier for Naive Bayes first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7rr0V-p2HOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Model\n",
        "nb_clas = Pipeline([('vect', CountVectorizer()), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', MultinomialNB())])\n",
        "\n",
        "nb_clas = nb_clas.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeYqupzj4LOb",
        "colab_type": "text"
      },
      "source": [
        "Let's test our model now and get the performance on the 15% testing set. We will be taking the testing set inputs and comparing the model outputs to the actual outputs for our testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYyrlphQ4XRC",
        "colab_type": "code",
        "outputId": "9ed04ff6-6f73-4d50-8ef1-84d443d65f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Dataset \n",
        "# Predictions \n",
        "nb_predicted = nb_clas.predict(X_test)\n",
        "\n",
        "# Performance\n",
        "np.mean(nb_predicted == y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7717572391907973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5mdNWQh4uGJ",
        "colab_type": "text"
      },
      "source": [
        "We got ~ 77.18% accuracy on the testing set which isn't bad for a start with naive classifier. \n",
        "\n",
        "Let's try the next model, compare the two and then we can start to fine-tune the chosen model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOWegrDa45T3",
        "colab_type": "text"
      },
      "source": [
        "## 2. Support Vector Machines (SVM)\n",
        "Time to try our second model, Support Vector Machines. We are going to try three variations of this model: \n",
        "\n",
        "\n",
        "*   Linear Kernel\n",
        "*   Polynomial Kernel\n",
        "*   Gaussian Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYGcEIR-NgMV",
        "colab_type": "text"
      },
      "source": [
        "### Linear Kernel\n",
        "We use the SGDClassifier under linear models with sklearn - the default is a SVM. This will be SVM with a linear kernel using Stochastic Gradient Descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qG5KNMWY9IfX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating pipeline\n",
        "linSvm_pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('svm-lin', SGDClassifier(loss='hinge', penalty='l2',random_state=42))])\n",
        "\n",
        "# Setting GridSearch Parameters\n",
        "linSVM_param_grid = {\n",
        "     'svm-lin__alpha': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
        "     'tfidf__use_idf': (True, False),\n",
        "     'vect__ngram_range': [(1,1), (1,2)]\n",
        "}\n",
        "\n",
        "linSVM_search = GridSearchCV(linSvm_pipe, linSVM_param_grid, scoring=scorers, refit='precision_score', return_train_score=True, n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imKhKpN6IpiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lin_svm = linSVM_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fIPvFPqxIeub",
        "colab": {}
      },
      "source": [
        "# Predictions on Test Set\n",
        "svm_lin_pred = lin_svm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTQuyzoNJMN6",
        "colab_type": "code",
        "outputId": "06a9e710-0033-4cda-b45b-4d6e5d768dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "linSVM_results = {'Metrics': ['Precision', 'Recall', 'Accuracy'],\n",
        "                  'Scores': [precision_score(y_test, svm_lin_pred, average='micro'),\n",
        "                             recall_score(y_test, svm_lin_pred, average='micro'),\n",
        "                             accuracy_score(y_test, svm_lin_pred)]}\n",
        "\n",
        "pd.DataFrame(linSVM_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metrics</th>\n",
              "      <th>Scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Precision</td>\n",
              "      <td>0.881793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Recall</td>\n",
              "      <td>0.881793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>0.881793</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Metrics    Scores\n",
              "0  Precision  0.881793\n",
              "1     Recall  0.881793\n",
              "2   Accuracy  0.881793"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB59fkq0IfXn",
        "colab_type": "code",
        "outputId": "9a417c52-152d-4c0e-fd65-b6ed4a81eb52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print('Best parameters:', linSVM_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters: {'svm-lin__alpha': 1e-05, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8lqyi4t6nlf",
        "colab_type": "text"
      },
      "source": [
        "With Linear SVM, we got ~88.1793% accuracy on our testing set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzeAqDxWdmi3",
        "colab_type": "text"
      },
      "source": [
        "### Polynomial Kernel\n",
        "Now, we'll the Polynomial Kernel to see if this works better than the linear kernel with classifying our headlines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY5l2eN1P_58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Pipeline\n",
        "polySVM_pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('svm-poly', SVC(kernel='poly', degree=2, n_jobs=-1)))])\n",
        "\n",
        "# Setting GridSearch Parameters\n",
        "polySVM_param_grid = {\n",
        "    'svm-poly__C': [0.01, 0.1, 1, 10],\n",
        "    'svm-poly__degree': [2,3],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'vect__ngram_range': [(1,1), (1,2)]\n",
        "}\n",
        "\n",
        "polySVM_search = GridSearchCV(polySVM_pipe, polySVM_param_grid, scoring=scorers, \n",
        "                              refit='precision_score', return_train_score=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpxPohZTKdAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "poly_svm = polySVM_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf1ImLEndmZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions on Test Set \n",
        "svm_poly_pred = poly_svm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR8VwsjcKlpS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "polySVM_results = {'Metrics': ['Precision', 'Recall', 'Accuracy'],\n",
        "                  'Scores': [precision_score(y_test, svm_poly_pred, average='micro'),\n",
        "                             recall_score(y_test, svm_poly_pred, average='micro'),\n",
        "                             accuracy_score(y_test, svm_poly_pred)]}\n",
        "\n",
        "pd.DataFrame(polySVM_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b73e-ADiKrPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Best parameters:', polySVM_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxiwWwOTdmQi",
        "colab_type": "text"
      },
      "source": [
        "With Polynomial SVM, we got ~79.83% accuracy on our testing set so the linear kernel worked better than polynomial of degree 2 with C=10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaBE4rO2dmEu",
        "colab_type": "text"
      },
      "source": [
        "### Gaussian Kernel\n",
        "Lastly, we will be trying the Gaussian kernel for SVM. We will be trying different parameters for C, and for gamma for this kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2Akw6HidlrL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the Model\n",
        "gausSVM_pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('svm-gaus', SVC(kernel='rbf'))])\n",
        "\n",
        "# Setting GridSearch Parameters for the pipeline\n",
        "gausSVM_param_grid = {\n",
        "    'svm-gaus__C': [0.001, 0.01, 0.1, 1, 10],\n",
        "    'svm-gaus__gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'vect__ngram_range': [(1,1), (1,2)]\n",
        "}\n",
        "\n",
        "gausSVM_search = GridSearchCV(gausSVM_pipe, gausSVM_param_grid, scoring=scorers, \n",
        "                              refit='precision_score', return_train_score=True,\n",
        "                              n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIoxtaRMCdBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gaus_svm = gausSVM_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0HDtYeKCbW2",
        "colab_type": "code",
        "outputId": "431d2243-5789-46e8-85b5-f3cdb544307f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "# Predictions on Test Set \n",
        "svm_gaus_pred = gaus_svm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-f3a06b2a3364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msvm_gaus_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgaus_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gaus_svm' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozW7fyERUo8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gausSVM_results = {'Metrics': ['Precision', 'Recall', 'Accuracy'],\n",
        "                  'Scores': [precision_score(y_test, svm_gaus_pred, average='micro'),\n",
        "                             recall_score(y_test, svm_gaus_pred, average='micro'),\n",
        "                             accuracy_score(y_test, svm_gaus_pred)]}\n",
        "\n",
        "pd.DataFrame(gausSVM_results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYcsBRDrLgDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Best parameters:', gausSVM_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7EVRAjGU9aP",
        "colab_type": "text"
      },
      "source": [
        "The Gaussion kernel had a accuracy of 83.7621 % on the testing set. Therefore with regards to the SVM, we have that the linear kernel worked best, then the Gaussian and then polynomial with degree 2.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUMgEdBMVcVV",
        "colab_type": "text"
      },
      "source": [
        "## 3. Logistic Regression\n",
        "Logistic regression is a simple model and easy to understand; it is extremely useful in binary classification but can easily be generalized to multiple classes like in our case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkBufiCjVcJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Pipeline\n",
        "logreg_clas = Pipeline([('vect', CountVectorizer()),\n",
        "                    ('tfidf', TfidfTransformer()),\n",
        "                    ('logreg', LogisticRegression(n_jobs=-1, max_iter= 500, penalty='l2'))])\n",
        "\n",
        "# Setting GridSearch Parameters for the pipeline\n",
        "logreg_param_grid = {\n",
        "    'logreg__C': [0.001, 0.01, 0.1, 1],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'vect__ngram_range': [(1,1), (1,2)]\n",
        "}\n",
        "\n",
        "logreg_search = GridSearchCV(logreg_clas, logreg_param_grid, scoring=scorers, \n",
        "                             refit='precision_score', return_train_score=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5JNaJ4wLWff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training the Model\n",
        "logreg_clas = logreg_search.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lbbijIKVbXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions on Test Set\n",
        "log_reg_pred = logreg_clas.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C45UNC6FLZSP",
        "colab_type": "code",
        "outputId": "a20b37bc-e00b-4026-bf1a-9d4e40d2535a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "logreg_results = {'Metrics': ['Precision', 'Recall', 'Accuracy'],\n",
        "                  'Scores': [precision_score(y_test, log_reg_pred, average='micro'),\n",
        "                             recall_score(y_test, log_reg_pred, average='micro'),\n",
        "                             accuracy_score(y_test, log_reg_pred)]}\n",
        "\n",
        "pd.DataFrame(logreg_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metrics</th>\n",
              "      <th>Scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Precision</td>\n",
              "      <td>0.858786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Recall</td>\n",
              "      <td>0.858786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>0.858786</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Metrics    Scores\n",
              "0  Precision  0.858786\n",
              "1     Recall  0.858786\n",
              "2   Accuracy  0.858786"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky-JofHzLdta",
        "colab_type": "code",
        "outputId": "a842e0a5-cceb-4ea5-dd65-3f38861413a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print('Best parameters:', logreg_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters: {'logreg__C': 1, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNN8i6jYXw_R",
        "colab_type": "text"
      },
      "source": [
        "Multinomial Logistic Regression has a 85.8786% accuracy with the testing set. \n",
        "\n",
        "This is higher than the Multinomial Naive Bayes, but lower than Linear SVM by ~3%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AHZUh32X9Tr",
        "colab_type": "text"
      },
      "source": [
        "## 4. Random Forest \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVkPilACX9Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the Pipeline\n",
        "rf_pipe = Pipeline([('vect', CountVectorizer()),\n",
        "                    ('tfidf', TfidfTransformer()),\n",
        "                    ('rf-clas', RandomForestClassifier(n_jobs=-1))])\n",
        "\n",
        "# Setting GridSearch Parameters\n",
        "rf_param_grid = {\n",
        "    'rf-clas__n_estimators': [100,200,300],\n",
        "    'rf-clas__max_depth': [3,5,10,25],\n",
        "    'rf-clas__max_features': [3, 5, 10, 25, 31],\n",
        "    'tfidf__use_idf': (True, False),\n",
        "    'vect__ngram_range': [(1,1), (1,2)]\n",
        "}\n",
        "\n",
        "rf_search = GridSearchCV(rf_pipe, rf_param_grid, scoring=scorers, \n",
        "                         refit='precision_score', return_train_score=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iiTTa57MVto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf_clas = rf_search.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-68cZkeX9BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predictions \n",
        "rf_pred = rf_clas.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzlLDeksMaaS",
        "colab_type": "code",
        "outputId": "6359178f-c922-4338-ab3b-6316bb224cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "rf_results = {'Metrics': ['Precision', 'Recall', 'Accuracy'],\n",
        "                  'Scores': [precision_score(y_test, rf_pred, average='micro'),\n",
        "                             recall_score(y_test, rf_pred, average='micro'),\n",
        "                             accuracy_score(y_test, rf_pred)]}\n",
        "\n",
        "pd.DataFrame(rf_results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metrics</th>\n",
              "      <th>Scores</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Precision</td>\n",
              "      <td>0.519556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Recall</td>\n",
              "      <td>0.519556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Accuracy</td>\n",
              "      <td>0.519556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Metrics    Scores\n",
              "0  Precision  0.519556\n",
              "1     Recall  0.519556\n",
              "2   Accuracy  0.519556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG8P9lefMcxH",
        "colab_type": "code",
        "outputId": "21e17750-d4d8-44b1-9709-2d80a21e9aa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print('Best parameters:', rf_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters: {'rf-clas__max_depth': 25, 'rf-clas__max_features': 31, 'rf-clas__n_estimators': 100, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 1)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5kQthFNX829",
        "colab_type": "text"
      },
      "source": [
        "Random Forest has the lowest accuracy on the test set with an accuray of 51.9476%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTUzRKZ7X8rh",
        "colab_type": "text"
      },
      "source": [
        "## Overall Comparison\n",
        "Now that we have trained all of our models,let's compare the accuracy on the testing set and then we will choose one model to focus on evaluating and fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwxt28V-X8g7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overall_models = pd.DataFrame({'Models': ['Multinomial Naive Bayes', 'Linear SVM',\n",
        "                                          'Polynomial SVM', 'Gaussian SVM', \n",
        "                                          'Logistic Regression', 'Random Forest'],\n",
        "                               'Training Set Accuracy': [nb_clas.score(X_train, y_train),\n",
        "                                                         lin_svm.score(X_train, y_train),\n",
        "                                                         poly_svm.score(X_train, y_train),\n",
        "                                                         gaus_svm.score(X_train, y_train),\n",
        "                                                         logreg_clas.score(X_train, y_train),\n",
        "                                                         rf_clas.score(X_train, y_train)],\n",
        "                               'Test Set Accuracy': [np.mean(nb_predicted == y_test),\n",
        "                                            np.mean(svm_lin_pred == y_test),\n",
        "                                            np.mean(svm_poly_pred == y_test),\n",
        "                                            np.mean(svm_gaus_pred == y_test),\n",
        "                                            np.mean(log_reg_pred == y_test),\n",
        "                                            np.mean(rf_pred == y_test)]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4kbsFB9KRDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "overall_models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Piq92DGGX8Q-",
        "colab_type": "text"
      },
      "source": [
        "We can see that logistic regression, polynomial SVM and Gaussian SVM are severely overfitting the data. The accuracy of random forest is too low in comparison to the other models so we take a look at Multinomial Naive Bayes and Linear SVM. \n",
        "\n",
        "In this case, Linear SVM has a higher test set accuracy so we will focus on fine-tuning this model and using it for our classification purposes. We notice that it is overfitted as well, so we will take a look at what we can do to fix this, keeping in mind that the goal is to add more twitter data so more data will be added to hopefully resolve some of the overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jLreUSy6vbD",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning Chosen Model\n",
        "\n",
        "We have chosen Linear SVM for our model. There are two steps we need to consider:\n",
        "1. Dealing with the overfitting \n",
        "2. Handling imbalanced classes "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtlZ3rLAepv0",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting \n",
        "\n",
        "We will first try and retrain our SVM model by reducing the features to see if this helps with our overfitting. \n",
        "\n",
        "The next step would be to gather more data, but that will be considered after we look at step 2. Two solutions are to try and find more news headline data which can be tailored to helping deal with the imbalanced classes as well, and the other solution is to use the Twitter data that I will be labelling to use for the transfer learning step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVDFFJUMAJKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Precision, Recall, F1 Score, Accuracy per Class\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "precision_recall_fscore_support(y_test, svm_lin_pred, beta=1.0, labels=labels, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ci9Gf4p_yPI",
        "colab_type": "text"
      },
      "source": [
        "### Imbalanced Classes\n",
        "We saw that we had approximately 10x more data for politics than we did for crimes so this could cause our model to be trained to classify politics articles better, or more often than crime. \n",
        "\n",
        "Let's begin by looking at our confusion matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjiwPhjYep2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conf_matrix = pd.DataFrame(confusion_matrix(y_test, svm_lin_pred))\n",
        "labels = ['Crime', 'Entertainment', 'Politics', 'Sports', 'Business']\n",
        "conf_matrix.columns = labels\n",
        "conf_matrix.index = labels\n",
        "\n",
        "conf_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8JVLnEtepjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classification_report(y_test, svm_lin_pred, target_names = news['category'].unique()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhC5wCVofVZD",
        "colab_type": "text"
      },
      "source": [
        "The majority of our discrepancies seem to be with crime and sports, and sports and politics. The next step is to check to look into how to handle imbalanced datasets and see if those preprocessing methods help with the model training since we had a lot of politics data, but substantially less crime data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46bAa4txi7Ad",
        "colab_type": "text"
      },
      "source": [
        "# Next Steps \n",
        "1. How do we handle overfitting and imbalanced dataset?\n",
        "2. Try to use different features (e.g. Word2Vec with logistic regression)\n",
        "3. Collect tweets and apply transfer learning with the model and train again on the new data\n",
        "4. Create visualizations \n",
        "\n",
        "# Extensions\n",
        "1. Try to learn deep learning and K-Nearest Neighbors (KNN) and see if these models are better"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hglB6VL-6qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}